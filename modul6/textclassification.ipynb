{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Механізм уваги для обробки тестів\n\nМеханізми уваги зробили революцію в обробці природної мови (NLP), дозволивши моделям зосередитися на найбільш релевантних частинах вхідної послідовності. У цьому посібнику розглядаються поняття уваги, багатоголової уваги та кодерів-трансформерів у PyTorch для задач NLP.\n\nРозуміння уваги:\n\nУявіть, що ви читаєте речення. Ви не приділяєте однакову увагу кожному слову, а зосереджуєтесь на найбільш важливих, щоб зрозуміти зміст. Аналогічно, увага в моделях НЛП дозволяє моделі концентруватися на певних частинах вхідної послідовності (наприклад, реченні), які є найбільш важливими для конкретного завдання, наприклад, аналізу настрою або машинного перекладу.\n\n\n<div>\n<img src=\"https://preview.redd.it/nrd3yld06rr91.png?width=761&format=png&auto=webp&s=76b11148418849a21304943898526dbdfb60052c\" width=\"500\"/>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Ключові поняття:\n\n* **Вектори запиту, ключа та значення:** Модель отримує на вхід 3 тензора: запит, ключ і значення. Вони відображають різні аспекти елементів послідовності.\n* **Ймовірність уваги:** Модель обчислює оцінку для кожної пари елементів у послідовності. Ця оцінка й відображає, наскільки важливим є один елемент (на основі його вектора ключа) для поточного елемента (на основі його вектора запиту).\n* **Зважена сума:** Використовуючи оцінки уваги, модель створює зважену суму векторів значень, ефективно фокусуючись на найбільш релевантних частинах послідовності.\n\n**Переваги уваги:** \n\n* **Довгострокові залежності:** Увага допомагає вловлювати довгострокові залежності в тексті, де віддалені слова можуть бути семантично пов'язані між собою. Це має вирішальне значення для таких завдань, як відповіді на запитання або аналіз настроїв.\n* **Розпаралелювання:** Обчислення уваги можна ефективно розпаралелити, що робить їх придатними для навчання на великих наборах даних за допомогою графічних процесорів.\n\n![](https://www.researchgate.net/publication/356271104/figure/fig5/AS:1090999354961922@1637125924695/The-general-process-of-attention-mechanism.png)","metadata":{}},{"cell_type":"markdown","source":"# Багатоголова увага\n\nСтандартний механізм уваги фокусується на одному аспекті взаємозв'язків між елементами. Багатоголова увага вирішує цю проблему, виконуючи увагу з декількох «голів», кожна з яких вивчає різні представлення взаємозв'язків. Це дозволяє моделі відображати більш глибоке розуміння вхідної послідовності.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*PiZyU-_J_nWixsTjXOUP7Q.png)","metadata":{}},{"cell_type":"markdown","source":"# Модель Трансформер\n\nТрансформаторна архітектура, популярний вибір для завдань НЛП, значною мірою покладається на механізми уваги. Кодер Transformer використовує шари багатоголової уваги з наступними мережами прямого поширення. Ці шари дозволяють моделі вивчати складні взаємозв'язки між елементами вхідної послідовності.\n\n![](https://quantdare.com/wp-content/uploads/2021/11/transformer_arch.png)","metadata":{}},{"cell_type":"markdown","source":"# Читання даних","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:21:13.957157Z","iopub.execute_input":"2024-04-23T22:21:13.957836Z","iopub.status.idle":"2024-04-23T22:21:17.734203Z","shell.execute_reply.started":"2024-04-23T22:21:13.957804Z","shell.execute_reply":"2024-04-23T22:21:17.733097Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/bbc-full-text-document-classification/bbc_data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:21:17.736133Z","iopub.execute_input":"2024-04-23T22:21:17.736557Z","iopub.status.idle":"2024-04-23T22:21:18.826715Z","shell.execute_reply.started":"2024-04-23T22:21:17.736528Z","shell.execute_reply":"2024-04-23T22:21:18.825715Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                data         labels\n0  Musicians to tackle US red tape  Musicians gro...  entertainment\n1  U2s desire to be number one  U2, who have won ...  entertainment\n2  Rocker Doherty in on-stage fight  Rock singer ...  entertainment\n3  Snicket tops US box office chart  The film ada...  entertainment\n4  Oceans Twelve raids box office  Oceans Twelve,...  entertainment","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Musicians to tackle US red tape  Musicians gro...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U2s desire to be number one  U2, who have won ...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Rocker Doherty in on-stage fight  Rock singer ...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Snicket tops US box office chart  The film ada...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Oceans Twelve raids box office  Oceans Twelve,...</td>\n      <td>entertainment</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nimport torchtext\n\n\nclass MyDataset(Dataset):\n    def __init__(self, X, y, encoding_dim, max_len=100):\n        self.X = X\n        self.y = y\n        self.max_len = max_len\n        \n        self.label_encoder = LabelEncoder().fit(y)\n        self.vocab = torchtext.vocab.GloVe(name='6B', dim=encoding_dim)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        label = self.label_encoder.transform([self.y.iloc[idx]])\n        label = torch.tensor(label)\n        \n        text = self.X.iloc[idx]\n        tokens = text.split()\n        \n        if len(tokens) > self.max_len:\n            tokens = tokens[:self.max_len]\n        else:\n            diff = self.max_len - len(tokens)\n            \n            tokens += ['<pad>'] * diff\n        \n        X = self.vocab.get_vecs_by_tokens(tokens, lower_case_backup=True)\n        \n        return X, label[0]\n    \n    \ndataset = MyDataset(df['data'], df['labels'], 50)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:50:13.495181Z","iopub.execute_input":"2024-04-23T22:50:13.495617Z","iopub.status.idle":"2024-04-23T22:50:14.167378Z","shell.execute_reply.started":"2024-04-23T22:50:13.495586Z","shell.execute_reply":"2024-04-23T22:50:14.166365Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"dataset[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:24:43.478809Z","iopub.execute_input":"2024-04-23T22:24:43.479400Z","iopub.status.idle":"2024-04-23T22:24:43.501736Z","shell.execute_reply.started":"2024-04-23T22:24:43.479370Z","shell.execute_reply":"2024-04-23T22:24:43.500792Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"torch.Size([100, 50])"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 16\ntrain_dl = DataLoader(dataset,  # датасет з даними\n                        batch_size=batch_size,  # кількість даних в одному пакеті\n                        shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:50:18.393679Z","iopub.execute_input":"2024-04-23T22:50:18.394446Z","iopub.status.idle":"2024-04-23T22:50:18.433079Z","shell.execute_reply.started":"2024-04-23T22:50:18.394415Z","shell.execute_reply":"2024-04-23T22:50:18.432224Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:24:43.516079Z","iopub.execute_input":"2024-04-23T22:24:43.516464Z","iopub.status.idle":"2024-04-23T22:24:43.949626Z","shell.execute_reply.started":"2024-04-23T22:24:43.516425Z","shell.execute_reply":"2024-04-23T22:24:43.947915Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      3\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass TextClassifier(nn.Module):\n    def __init__(self, encoding_dim, num_classes):\n        super().__init__()\n\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=encoding_dim, nhead=2, batch_first=True, dim_feedforward=64),\n            num_layers=1\n        )\n\n        self.flatten = nn.Flatten()\n\n        self.linear1 = nn.Linear(100*50, num_classes)\n\n    def forward(self, x):\n        out = self.encoder(x)\n        out = self.flatten(out)\n        out = self.linear1(out)\n        return out\n\n\n    def predict(self, X, device='cpu'):\n        X = torch.FloatTensor(np.array(X)).to(device)\n\n        with torch.no_grad():\n            y_pred = F.softmax(self.forward(X), dim=-1)\n\n        return y_pred.cpu().numpy()\n\n\nmodel = TextClassifier(50, 5).to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:50:29.873799Z","iopub.execute_input":"2024-04-23T22:50:29.874605Z","iopub.status.idle":"2024-04-23T22:50:29.899352Z","shell.execute_reply.started":"2024-04-23T22:50:29.874570Z","shell.execute_reply":"2024-04-23T22:50:29.898263Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TextClassifier(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n        )\n        (linear1): Linear(in_features=50, out_features=64, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=64, out_features=50, bias=True)\n        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear1): Linear(in_features=5000, out_features=5, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:26:38.251751Z","iopub.execute_input":"2024-04-23T22:26:38.252480Z","iopub.status.idle":"2024-04-23T22:26:38.286299Z","shell.execute_reply.started":"2024-04-23T22:26:38.252446Z","shell.execute_reply":"2024-04-23T22:26:38.285039Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"],"ename":"NameError","evalue":"name 'X' is not defined","output_type":"error"}]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:50:39.602621Z","iopub.execute_input":"2024-04-23T22:50:39.603324Z","iopub.status.idle":"2024-04-23T22:50:41.725960Z","shell.execute_reply.started":"2024-04-23T22:50:39.603289Z","shell.execute_reply":"2024-04-23T22:50:41.725168Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"X, y = next(iter(train_dl))\n\nmodel(X.to(device))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:50:48.289635Z","iopub.execute_input":"2024-04-23T22:50:48.290522Z","iopub.status.idle":"2024-04-23T22:50:48.619688Z","shell.execute_reply.started":"2024-04-23T22:50:48.290486Z","shell.execute_reply":"2024-04-23T22:50:48.618713Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.5110, -0.5174, -0.5850,  0.3872, -0.7153],\n        [-0.4042, -0.6381,  1.3053,  0.1748, -0.1258],\n        [-0.1710, -0.7105,  1.2146,  0.1666, -0.4207],\n        [ 0.0259, -0.9879,  0.2146,  0.1476, -0.6036],\n        [-0.9961, -0.7443,  0.7241, -0.2904, -1.0233],\n        [ 0.7888, -0.4488, -0.1858, -0.4195, -0.5415],\n        [-0.1221, -1.2209,  0.8606, -0.2441, -0.8110],\n        [-0.4983, -1.0048, -0.0506,  0.5037, -0.8385],\n        [ 0.8026, -0.5030, -0.0791, -0.0119, -0.4100],\n        [-0.9414, -0.5763,  0.8781,  0.2530, -1.0987],\n        [-0.0884, -0.3518,  0.4113, -0.1300, -0.3059],\n        [-0.1724, -0.6278,  0.8081,  0.1573, -0.1922],\n        [-0.9416, -0.4749,  1.8490,  0.4080, -0.4054],\n        [-0.1918, -0.6672,  1.0119, -0.1743, -0.6668],\n        [ 0.2240, -0.4512,  1.1516,  0.5543, -0.2016],\n        [ 0.0121, -0.3552,  0.5433, -0.3864, -0.2506]], device='cuda:0',\n       grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"import time\n\ndef train(model, optimizer, loss_fn, train_dl, val_dl,\n          metrics=None, metrics_name=None, epochs=20, device='cpu', task='regression'):\n    '''\n    Runs training loop for classification problems. Returns Keras-style\n    per-epoch history of loss and accuracy over training and validation data.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Neural network model\n    optimizer : torch.optim.Optimizer\n        Search space optimizer (e.g. Adam)\n    loss_fn :\n        Loss function (e.g. nn.CrossEntropyLoss())\n    train_dl :\n        Iterable dataloader for training data.\n    val_dl :\n        Iterable dataloader for validation data.\n    metrics: list\n        List of sklearn metrics functions to be calculated\n    metrics_name: list\n        List of matrics names\n    epochs : int\n        Number of epochs to run\n    device : string\n        Specifies 'cuda' or 'cpu'\n    task : string\n        type of problem. It can be regression, binary or multiclass\n\n    Returns\n    -------\n    Dictionary\n        Similar to Keras' fit(), the output dictionary contains per-epoch\n        history of training loss, training accuracy, validation loss, and\n        validation accuracy.\n    '''\n\n    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n          (type(model).__name__, type(optimizer).__name__,\n           optimizer.param_groups[0]['lr'], epochs, device))\n\n    metrics = metrics if metrics else []\n    metrics_name = metrics_name if metrics_name else [metric.__name__ for metric in metrics]\n\n    history = {} # Collects per-epoch loss and metrics like Keras' fit().\n    history['loss'] = []\n    history['val_loss'] = []\n    for name in metrics_name:\n        history[name] = []\n        history[f'val_{name}'] = []\n\n    start_time_train = time.time()\n\n    for epoch in range(epochs):\n\n        # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n        start_time_epoch = time.time()\n\n        model.train()\n        history_train = {name: 0 for name in ['loss']+metrics_name}\n\n        for batch in train_dl:\n            x    = batch[0].to(device)\n            y    = batch[1].to(device)\n            y_pred = model(x)\n            loss = loss_fn(y_pred, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            y_pred = y_pred.detach().cpu().numpy()\n            y = y.detach().cpu().numpy()\n\n\n            history_train['loss'] += loss.item() * x.size(0)\n            for name, func in zip(metrics_name, metrics):\n              try:\n                  history_train[name] += func(y, y_pred) * x.size(0)\n              except:\n                  if task == 'binary': y_pred_ = y_pred.round()\n                  elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n                  history_train[name] += func(y, y_pred_) * x.size(0)\n\n        for name in history_train:\n            history_train[name] /= len(train_dl.dataset)\n\n\n        # --- EVALUATE ON VALIDATION SET -------------------------------------\n        model.eval()\n        history_val = {'val_' + name: 0 for name in metrics_name+['loss']}\n\n        with torch.no_grad():\n            for batch in val_dl:\n                x    = batch[0].to(device)\n                y    = batch[1].to(device)\n                y_pred = model(x)\n                loss = loss_fn(y_pred, y)\n\n                y_pred = y_pred.cpu().numpy()\n                y = y.cpu().numpy()\n\n                history_val['val_loss'] += loss.item() * x.size(0)\n                for name, func in zip(metrics_name, metrics):\n                    try:\n                        history_val['val_'+name] += func(y, y_pred) * x.size(0)\n                    except:\n                        if task == 'binary': y_pred_ = y_pred.round()\n                        elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n\n                        history_val['val_'+name] += func(y, y_pred_) * x.size(0)\n\n        for name in history_val:\n            history_val[name] /= len(val_dl.dataset)\n\n        # PRINTING RESULTS\n\n        end_time_epoch = time.time()\n\n        for name in history_train:\n            history[name].append(history_train[name])\n            history['val_'+name].append(history_val['val_'+name])\n\n        total_time_epoch = end_time_epoch - start_time_epoch\n\n        print(f'Epoch {epoch+1:4d} {total_time_epoch:4.0f}sec', end='\\t')\n        for name in history_train:\n            print(f'{name}: {history[name][-1]:10.3g}', end='\\t')\n            print(f\"val_{name}: {history['val_'+name][-1]:10.3g}\", end='\\t')\n        print()\n\n    # END OF TRAINING LOOP\n\n    end_time_train       = time.time()\n    total_time_train     = end_time_train - start_time_train\n    print()\n    print('Time total:     %5.2f sec' % (total_time_train))\n\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:50:53.280058Z","iopub.execute_input":"2024-04-23T22:50:53.280774Z","iopub.status.idle":"2024-04-23T22:50:53.306120Z","shell.execute_reply.started":"2024-04-23T22:50:53.280737Z","shell.execute_reply":"2024-04-23T22:50:53.304942Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score\n\nhistory = train(model, optimizer, loss_fn, train_dl, train_dl,\n                epochs=5,\n                metrics=[accuracy_score],\n                device=device,\n                task='multiclass')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T22:50:55.178627Z","iopub.execute_input":"2024-04-23T22:50:55.179208Z","iopub.status.idle":"2024-04-23T22:51:13.362123Z","shell.execute_reply.started":"2024-04-23T22:50:55.179176Z","shell.execute_reply":"2024-04-23T22:51:13.361047Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"train() called: model=TextClassifier, opt=Adam(lr=0.001000), epochs=5, device=cuda\n\nEpoch    1    4sec\tloss:      0.372\tval_loss:     0.0388\taccuracy_score:       0.87\tval_accuracy_score:      0.989\t\nEpoch    2    4sec\tloss:     0.0327\tval_loss:    0.00753\taccuracy_score:      0.991\tval_accuracy_score:          1\t\nEpoch    3    4sec\tloss:    0.00591\tval_loss:    0.00268\taccuracy_score:          1\tval_accuracy_score:          1\t\nEpoch    4    4sec\tloss:     0.0026\tval_loss:     0.0016\taccuracy_score:          1\tval_accuracy_score:          1\t\nEpoch    5    4sec\tloss:    0.00146\tval_loss:   0.000969\taccuracy_score:          1\tval_accuracy_score:          1\t\n\nTime total:     18.11 sec\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}